{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f75a3e9",
   "metadata": {},
   "source": [
    "### Clasificación   \n",
    "La clasificación es una tarea de aprendizaje supervisado en ML que consiste en asignar una etiqueta de clase (variable categórica discreta) a un elemento desconocido, a partir de un conjunto de variables características. \n",
    "\n",
    "Dado un conjunto de datos de entrenamiento etiquetados (con pares de características y etiquetas objetivo), un algoritmo de clasificación aprende una función que mapea las entradas a una de las clases predefinidas.\n",
    "\n",
    "Este enfoque permite construir modelos para clasificación binaria (2 clases) o multiclase (más de 2 clases).\n",
    "\n",
    "![Ejemplos](./img/ejemplos_clasificacion.png)\n",
    "\n",
    "![Algoritmos](./img/algoritmos_clasificacion.png)\n",
    "\n",
    "##### Árbol de Decisión\n",
    "Un Árbol de Decisión es un modelo de Machine Learning que aprende a clasificar o predecir dividiendo los datos en subgrupos mediante preguntas simples sobre las características (como “¿edad > 30?” o “¿tiene palabra ‘gratis’?”), eligiendo en cada paso la pregunta que mejor separa las clases, hasta llegar a hojas que contienen la predicción final. Un árbol de decisión se puede construir considerando los atributos uno por uno. Primero, elige un atributo de nuestro conjunto de datos. Calcula la significancia del atributo en la división de los datos. A continuación, divide los datos según el valor del mejor atributo. Luego, ve a cada rama y repítelo para el resto de los atributos. Los árboles de decisión se construyen utilizando particionamiento recursivo para clasificar los datos. El algoritmo elige la característica más predictiva para dividir los datos. Lo más importante es determinar qué atributo es el mejor o más predictivo para dividir los datos según la característica. \n",
    "\n",
    "La entropía es la cantidad de desorden de la información, o la cantidad de aleatoriedad en los datos. La entropía en el nodo depende de cuántos datos aleatorios hay en ese nodo, y se calcula para cada nodo. \n",
    "\n",
    "La respuesta es, el árbol con mayor ganancia de información después de la división. \n",
    "\n",
    "La ganancia de información es la información que puede aumentar el nivel de certeza después de la división. Podemos pensar en la ganancia de información y la entropía como opuestos; a medida que la entropía o la cantidad de aleatoriedad disminuye, la ganancia de información o la cantidad de certeza aumenta y viceversa. Por lo tanto, construir un árbol de decisión se trata de encontrar atributos que devuelvan la mayor ganancia de información. \n",
    "\n",
    "##### K-Nearest Neighbors\n",
    "K-Nearest Neighbors (KNN) es un algoritmo simple y sin entrenamiento: no crea reglas ni modelos.\n",
    "\n",
    "Guarda todos los datos de entrenamiento (como una base de datos).\n",
    "Cuando llega un dato nuevo, mide su distancia (ej. euclidiana) a todos los puntos guardados.\n",
    "Escoge los K puntos más cercanos (los “vecinos”).\n",
    "Clasifica con la etiqueta más común de esos K vecinos (voto mayoritario).\n",
    "Resultado: “Si te pareces a gatos, eres gato”. Ideal para patrones locales, pero lento y sensible a escalas.\n",
    "\n",
    "Se basa en este paradigma. Los casos similares con las mismas etiquetas de clase están cerca unos de otros.\n",
    "\n",
    "En una frase: “Tú eres como tus vecinos más cercanos.”"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
