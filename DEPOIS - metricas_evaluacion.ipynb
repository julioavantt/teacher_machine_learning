{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d429012",
   "metadata": {},
   "source": [
    "### Métricas de evaluación \n",
    "Las métricas de evaluación desempeñan un papel clave en el desarrollo de un modelo, ya que proporcionan información sobre áreas que podrían requerir mejoras.\n",
    "\n",
    "![Métricas de Evaluación](./img/metricas.png)\n",
    "\n",
    "Hablaremos solo de tres de ellas: el índice de Jaccard, la puntuación F1 y la pérdida logarítmica.\n",
    "\n",
    "El índice de Jaccard y también conocido como el coeficiente de similitud de Jaccard es el tamaño de la intersección dividido por el tamaño de la unión de dos conjuntos de etiquetas. Si todo el conjunto de etiquetas predichas para una muestra coincide estrictamente con el conjunto verdadero de etiquetas, entonces la precisión del subconjunto es 1.0. \n",
    "\n",
    "Otra forma de ver la precisión de los clasificadores es observar una matriz de confusión. Por ejemplo, supongamos que nuestro conjunto de prueba tiene solo 40 filas. Esta matriz muestra las predicciones correctas e incorrectas en comparación con las etiquetas reales. Cada fila de la matriz de confusión muestra las etiquetas verdaderas en el conjunto de prueba y las columnas muestran las etiquetas predichas por el clasificador. Una buena cosa sobre la matriz de confusión es que muestra la capacidad del modelo para predecir o separar correctamente las clases. \n",
    "\n",
    "Basándonos en el conteo de cada sección, podemos calcular la precisión y la recuperación de cada etiqueta. La precisión es una medida de la exactitud dado que se ha predicho una etiqueta de clase. Se define como precisión igual a verdaderos positivos dividido por verdaderos positivos más falsos positivos. Y la recuperación es la tasa de verdaderos positivos. Se define como recuperación igual a verdaderos positivos dividido por verdaderos positivos más falsos negativos. Así que podemos calcular la precisión y la recuperación de cada clase. \n",
    "\n",
    "Ahora estamos en posición de calcular las puntuaciones F1 para cada etiqueta basándonos en la precisión y la recuperación de esa etiqueta. La puntuación F1 es el promedio armónico de la precisión y la recuperación, donde una puntuación F1 alcanza su mejor valor en 1, lo que representa una precisión y recuperación perfectas, y su peor en 0. Es una buena manera de mostrar que un clasificador tiene un buen valor tanto para la recuperación como para la precisión. \n",
    "\n",
    "En la primera instancia, la salida de un clasificador es la probabilidad de una etiqueta de clase en lugar de la etiqueta. \n",
    "\n",
    "Podemos calcular la pérdida logarítmica para cada fila utilizando la ecuación de pérdida logarítmica, que mide qué tan lejos está cada predicción de la pérdida logarítmica real. Y podemos calcular la pérdida logarítmica para cada fila utilizando la ecuación de pérdida logarítmica, que mide qué tan lejos está cada predicción de la pérdida logarítmica real. Luego calculamos la pérdida logarítmica promedio a través de todas las filas del conjunto de prueba. Es obvio que los clasificadores más ideales tienen valores de pérdida logarítmica progresivamente más pequeños.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
